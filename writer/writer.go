
// converted golang begin

package writer

import(
//	"errors"
	"fmt"

	"encoding/json"

 _core	"nodejs/core"
 JSON	"nodejs/json"
	"nodejs/console"

 .	"github.com/byteball/go-byteballcore/types"

)

import(
// _		"lodash"
//		"async"
		"github.com/byteball/go-byteballcore/constants"
		"github.com/byteball/go-byteballcore/conf"
		"github.com/byteball/go-byteballcore/storage"
		"github.com/byteball/go-byteballcore/db"
 objectHash	"github.com/byteball/go-byteballcore/object_hash"
		"github.com/byteball/go-byteballcore/mutex"
		"github.com/byteball/go-byteballcore/main_chain"
 Definition	"github.com/byteball/go-byteballcore/definition"
 eventBus	"github.com/byteball/go-byteballcore/event_bus"
		"github.com/byteball/go-byteballcore/profiler"
)

type(
	DBConnT		= db.DBConnT

	DBParamsT	= db.DBParamsT
	DBQuerysT	= db.DBQuerysT

	DBQueryResultT	= db.DBQueryResultT
	refDBQueryResultT = *DBQueryResultT

	ObjJointT struct{
		Unit	UnitObjectT			`json:"unit"`
		Ball	BallT				`json:"ball"`

		Skiplist_units UnitsT			`json:"skiplist_units,omitempty"`
	}
	refObjJointT	= *ObjJointT

	ObjValidationStateT struct{
		ArrAdditionalQueries	DBQuerysT	`json:"arrAdditionalQueries"`
		ArrDoubleSpendInputs	DoubleSpendInputsT  `json:"arrDoubleSpendInputs"`
		ArrInputKeys		[]string	`json:"arrInputKeys"`
		Max_parent_limci	MCIndexT	`json:"max_parent_limci"`
		Last_ball_mci		MCIndexT	`json:"last_ball_mci"`
		Max_known_mci		MCIndexT	`json:"max_known_mci"`
		Witnessed_level		LevelT		`json:"witnessed_level"`
		Best_parent_unit	UnitT		`json:"best_parent_unit,omitempty"`
		ArrAddressesWithForkedPath  AddressesT	`json:"arrAddressesWithForkedPath"`
		Unit_hash_to_Sign	interface{}	`json:"unit_hash_to_sign"`
//		Unit_hash_to_Sign	[32]byte	`json:"unit_hash_to_sign"`
		Sequence		SequenceT	`json:"sequence,omitempty"`
	}
	refObjValidationStateT = *ObjValidationStateT

	DoubleSpendInputT struct{
		message_index		MessageIndexT	`json:"message_index"`
		input_index		InputIndexT	`json:"input_index"`
	}
	DoubleSpendInputsT	= []DoubleSpendInputT

	SequenceT		string

	preCommitCallbackT	func (*DBConnT)

	UnitObjectT struct{
		Unit		UnitT			`json:"unit"`
		Version		string			`json:"version"`
		Alt		string			`json:"alt"`

		Witness_list_unit UnitT			`json:"witness_list_unit,omitempty"`
		Last_ball_unit	UnitT			`json:"last_ball_unit,omitempty"`
		Last_ball	BallT			`json:"last_ball,omitempty"`

		Headers_commission  int			`json:"headers_commission,omitempty"`
		Payload_commission  int			`json:"payload_commission,omitempty"`
		Timestamp	int64			`json:"timestamp,omitempty"`

		Parent_units	UnitsT			`json:"parent_units,omitempty"`
		Authors		AuthorsT		`json:"authors,omitempty"`
		Messages	MessagesT		`json:"messages,omitempty"`

		// [fyi] field below were not observed
		Main_chain_index MCIndexT		`json:"main_chain_index,omitempty"`

		Witnesses	AddressesT		`json:"witnesses,omitempty"`

		Content_hash	*ContentHashT		`json:"content_hash,omitempty"`
		Earned_headers_commission_recipients EHCRsT  `json:"earned_headers_commission_recipients,omitempty"`
	}

	EHCRT struct{
		Address		AddressT		`json:"address"`
		Earned_headers_commission_share EHCST	`json:"earned_headers_commission_share"`
	}
	// [fyi] integer percent
	EHCST		int

	EHCRsT		= []EHCRT

	refAddressT	= *AddressT

	XPropsT		= storage.XPropsT
)


var	count_writes int = 0
var	count_units_in_prev_analyze int = 0


//func saveJoint_sync(objJoint objJointT, objValidationState objValidationStateT, preCommitCallback preCommitCallbackT) ErrorT {
func SaveJoint_sync(objJoint refObjJointT, objValidationState refObjValidationStateT, preCommitCallback_sync preCommitCallbackT) ErrorT {
	var(
//		determineInputAddressFromSrcOutput_sync func (asset AssetT, denomination denominationT, input inputT) AddressT
		determineInputAddressFromSrcOutput_sync func (asset AssetT, denomination DenominationT, input InputT) refAddressT
//		addInlinePaymentQueries_sync func () 
		addInlinePaymentQueries_sync func () ErrorT
		updateBestParent_sync func () ErrorT
//		determineMaxLevel_sync func () 
		determineMaxLevel_sync func () LevelT
		updateLevel_sync func () ErrorT
		updateWitnessedLevel_sync func () ErrorT
		updateWitnessedLevelByWitnesslist_sync func (arrWitnesses AddressesT) ErrorT
	)
	
//	objUnit := objJoint.unit
	objUnit := objJoint.Unit
//	console.Log("saving unit " + objUnit.unit)
	console.Log("saving unit %s", objUnit.Unit)
	profiler.Start()
	conn := /* await */
//	db.takeConnectionFromPool_sync()
	db.TakeConnectionFromPool_sync()
	// << flattened continuation for db.takeConnectionFromPool:24:1
//	arrQueries := AsyncFunctorsT{}
	conn.ResetAddedQueries()
//	conn.AddQuery(arrQueries, "BEGIN")
	// [fyi] .BeginTransaction is invoked before .ExecuteAddedQueries
	
	// additional queries generated by the validator, used only when received a doublespend
//	for i := 0; i < len(objValidationState.arrAdditionalQueries); i++ {
//		objAdditionalQuery := objValidationState.arrAdditionalQueries[i]
	for _, objAdditionalQuery := range objValidationState.ArrAdditionalQueries {
//		console.Log("----- applying additional queries: " + objAdditionalQuery.sql)
//		conn.AddQuery(objAdditionalQuery.sql, objAdditionalQuery.params)
		console.Log("----- applying additional queries: %v", objAdditionalQuery.Sql)
		conn.AddQuery(objAdditionalQuery.Sql, objAdditionalQuery.Params)
	}
	
	fields := "unit, version, alt, witness_list_unit, last_ball_unit, headers_commission, payload_commission, sequence, content_hash"
	values := "?,?,?,?,?,?,?,?,?"
/**
                var params = [objUnit.unit, 
			objUnit.version, 
			objUnit.alt, 
			objUnit.witness_list_unit, 
			objUnit.last_ball_unit,
                        objUnit.headers_commission || 0, 
			objUnit.payload_commission || 0, 
			objValidationState.sequence, 
			objUnit.content_hash];
 **/
	queryParams := DBParamsT{
		objUnit.Unit,
		objUnit.Version,
		objUnit.Alt,
		objUnit.Witness_list_unit.OrNull(),
		objUnit.Last_ball_unit.OrNull(),
		objUnit.Headers_commission,
		objUnit.Payload_commission,
		objValidationState.Sequence,
		objUnit.Content_hash,
	}

//	if conf.bLight {
	if conf.IsLight {
		fields += ", main_chain_index, creation_date"
//		values += ",?," + conn.getFromUnixTime("?")
		values += ",?," + conn.GetFromUnixTime("?")
		queryParams = append(queryParams, objUnit.Main_chain_index)
	}
	conn.AddQuery("INSERT INTO units (" + fields + ") VALUES (" + values + ")", queryParams)
	
//	if objJoint.ball && ! conf.bLight {
	if ! objJoint.Ball.IsNull() && ! conf.IsLight {
		conn.AddQuery("INSERT INTO balls (ball, unit) VALUES(?,?)", DBParamsT{
			objJoint.Ball,
			objUnit.Unit,
		})
		conn.AddQuery("DELETE FROM hash_tree_balls WHERE ball=? AND unit=?", DBParamsT{
			objJoint.Ball,
			objUnit.Unit,
		})
//		if objJoint.skiplist_units {
		if objJoint.Skiplist_units != nil {
//		if true {
			for i := 0; i < len(objJoint.Skiplist_units); i++ {
				conn.AddQuery("INSERT INTO skiplist_units (unit, skiplist_unit) VALUES (?,?)", DBParamsT{
					objUnit.Unit,
					objJoint.Skiplist_units[i],
				})
			}
		}
	}
	
//	if objUnit.parent_units {
	if objUnit.Parent_units != nil {
//	if true {
		for i := 0; i < len(objUnit.Parent_units); i++ {
			conn.AddQuery("INSERT INTO parenthoods (child_unit, parent_unit) VALUES(?,?)", DBParamsT{
				objUnit.Unit,
				objUnit.Parent_units[i],
			})
		}
	}
	
//	bGenesis := storage.isGenesisUnit(objUnit.unit)
	bGenesis := storage.IsGenesisUnit(objUnit.Unit)
	if bGenesis {
		conn.AddQuery("UPDATE units SET is_on_main_chain=1, main_chain_index=0, is_stable=1, level=0, witnessed_level=0 \n" +
			"				WHERE unit=?", DBParamsT{ objUnit.Unit })
	} else {
		// .. not flattening for conn.AddQuery
//		conn.AddQueryCb(arrQueries, "UPDATE units SET is_free=0 WHERE unit IN(?)", DBParamsT{ objUnit.parent_units }, func (result resultT) {
		queryParams := DBParamsT{}
		pusSql := queryParams.AddUnits(objUnit.Parent_units)
		conn.AddQueryCb("UPDATE units SET is_free=0 WHERE unit IN("+ pusSql +")", queryParams, func (result refDBQueryResultT) {
			// in sqlite3, result.affectedRows actually returns the number of _matched_ rows
			count_consumed_free_units := result.AffectedRows
//			console.log(count_consumed_free_units + " free units consumed")
			console.Log("%d free units consumed", count_consumed_free_units)
			// .. not flattening for Array.forEach
//			for parent_unit, _ := range objUnit.parent_units {
			for _, parent_unit := range objUnit.Parent_units {
//				if storage.AssocUnstableUnits[parent_unit] {
				if _, _exists := storage.AssocUnstableUnits[parent_unit]; _exists {
//					storage.AssocUnstableUnits[parent_unit].is_free = 0
					storage.AssocUnstableUnits[parent_unit].Is_free = 0
				}
			}
		})
	}
	
//	if Array.isArray(objUnit.Witnesses) {
	if objUnit.Witnesses != nil {
//		for i := 0; i < len(objUnit.Witnesses); i++ {
//			address := objUnit.Witnesses[i]
		for _, address := range objUnit.Witnesses {
			conn.AddQuery("INSERT INTO unit_witnesses (unit, address) VALUES(?,?)", DBParamsT{
				objUnit.Unit,
				address,
			})
		}
		conn.AddQuery("INSERT " + conn.GetIgnore() + " INTO witness_list_hashes (witness_list_unit, witness_list_hash) VALUES (?,?)", DBParamsT{
			objUnit.Unit,
//			objectHash.getBase64Hash(objUnit.witnesses),
			objectHash.GetBase64Hash(objUnit.Witnesses),
		})
	}
	
	arrAuthorAddresses := AddressesT{}
//	for i := 0; i < len(objUnit.Authors); i++ {
//		author := objUnit.Authors[i]
	for _, author := range objUnit.Authors {
		arrAuthorAddresses = append(arrAuthorAddresses, author.Address)
		definition := author.Definition
//		definition_chash := nil
		definition_chash := CHashT_Null
//		if definition {
		if definition != nil {
			// IGNORE for messages out of sequence
//			console.Log("definition %#v\n", definition)
		{{
			jsdef, _ := json.MarshalIndent(definition, "", "  ")
			console.Log("address %s\ndefinition --[[\n%s\n]]--\n", author.Address, jsdef)
		}}
//			definition_chash = objectHash.getChash160(definition)
			definition_chash = objectHash.GetChash160(definition)
			hasReferences := 0
//			if Definition.hasReferences(definition) { hasReferences = 1 }
			if Definition.HasReferences(definition) { hasReferences = 1 }
			conn.AddQuery("INSERT " + conn.GetIgnore() + " INTO definitions (definition_chash, definition, has_references) VALUES (?,?,?)", DBParamsT{
				definition_chash,
				JSON.Stringify(definition),
				hasReferences,
			})
			// actually inserts only when the address is first used.
			// if we change keys and later send a unit signed by new keys, the address is not inserted. 
			// Its definition_chash was updated before when we posted change-definition message.
//			if definition_chash == author.address {
			if definition_chash == CHashT(author.Address) {
				conn.AddQuery("INSERT " + conn.GetIgnore() + " INTO addresses (address) VALUES(?)", DBParamsT{ author.Address })
			}
		} else {
//			if objUnit.Content_hash {
			if objUnit.Content_hash != nil {
				conn.AddQuery("INSERT " + conn.GetIgnore() + " INTO addresses (address) VALUES(?)", DBParamsT{ author.Address })
			}
		}
		conn.AddQuery("INSERT INTO unit_authors (unit, address, definition_chash) VALUES(?,?,?)", DBParamsT{
			objUnit.Unit,
			author.Address,
			definition_chash.OrNull(),
		})
		if bGenesis {
			conn.AddQuery("UPDATE unit_authors SET _mci=0 WHERE unit=?", DBParamsT{ objUnit.Unit })
		}
//		if ! objUnit.Content_hash {
		if ! (objUnit.Content_hash != nil) {
//			for path := range author.Authentifiers {
			for path := range author.Authentifiers {
				conn.AddQuery("INSERT INTO authentifiers (unit, address, path, authentifier) VALUES(?,?,?,?)", DBParamsT{
					objUnit.Unit,
					author.Address,
					path,
					author.Authentifiers[path],
				})
			}
		}
	}
	
//	if ! objUnit.Content_hash {
	if ! (objUnit.Content_hash != nil) {
//		for i := 0; i < len(objUnit.Messages); i++ {
//			message := objUnit.Messages[i]
		for i, message := range objUnit.Messages {
			
//			text_payload := nil
			text_payload := interface{}(nil)
			if message.App == "text" {
				//text_payload = message.Payload
				text_payload = message.PayloadText
			} else {
				if message.App == "data" || message.App == "profile" || message.App == "attestation" || message.App == "definition_template" {
					// [fyi] no can go back in golang land
					//text_payload = JSON.Stringify(message.Payload)
					text_payload = message.PayloadText
				}
			}
			
			conn.AddQuery("INSERT INTO messages \n" +
				"	(unit, message_index, app, payload_hash, payload_location, payload, payload_uri, payload_uri_hash) VALUES(?,?,?,?,?,?,?,?)", DBParamsT{
				objUnit.Unit,
				i,
				message.App,
				message.Payload_hash,
				message.Payload_location,
				text_payload,
				message.Payload_uri,
				message.Payload_uri_hash,
			})
			
			if message.Payload_location == "inline" {
//				[*SwitchStatement*]
				switch message.App {

				case "address_definition_change":
					definition_chash := message.Payload.Definition_chash
					address := message.Payload.Address
					if address.IsNull() {
						address = objUnit.Authors[0].Address
					}
					conn.AddQuery("INSERT INTO address_definition_changes (unit, message_index, address, definition_chash) VALUES(?,?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						address,
						definition_chash,
					})
					//break

				case "poll":
					poll := message.Payload
					conn.AddQuery("INSERT INTO polls (unit, message_index, question) VALUES(?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						poll.Question,
					})
					for j := range poll.Choices {
						conn.AddQuery("INSERT INTO poll_choices (unit, choice_index, choice) VALUES(?,?,?)", DBParamsT{
							objUnit.Unit,
							j,
							poll.Choices[j],
						})
					}
					//break

				case "vote":
					vote := message.Payload
					conn.AddQuery("INSERT INTO votes (unit, message_index, poll_unit, choice) VALUES (?,?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						vote.Unit,
						vote.Choice,
					})
					//break

				case "attestation":
					attestation := message.Payload
					conn.AddQuery("INSERT INTO attestations (unit, message_index, attestor_address, address) VALUES(?,?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						objUnit.Authors[0].Address,
						attestation.Address,
					})
					for field, value := range attestation.Profile {
						if len(field) <= constants.MAX_PROFILE_FIELD_LENGTH && true && len(value) <= constants.MAX_PROFILE_VALUE_LENGTH {
							conn.AddQuery("INSERT INTO attested_fields (unit, message_index, attestor_address, address, field, value) VALUES(?,?, ?,?, ?,?)", DBParamsT{
								objUnit.Unit,
								i,
								objUnit.Authors[0].Address,
								attestation.Address,
								field,
								value,
							})
						}
					}
					//break

				case "asset":
					boolToInt := func (b bool) int {
						if b { return 1 }
						return 0
					}
					objToJsonOrNull := func (obj interface{}) *string {
						if obj == nil { return nil }
						// [tbd] order of fields is not deterministic!!! [or is it?..]
						json := JSON.Stringify(obj)
						return &json
					}
					asset := message.Payload
					conn.AddQuery("INSERT INTO assets (unit, message_index, \n"+
						"cap, is_private, is_transferrable, auto_destroy, fixed_denominations, \n"+
						"issued_by_definer_only, cosigned_by_definer, spender_attested, \n"+
						"issue_condition, transfer_condition) VALUES(?,?,?,?,?,?,?,?,?,?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						asset.Cap,
						boolToInt(asset.Is_private),
						boolToInt(asset.Is_transferrable),
						boolToInt(asset.Auto_destroy),
						boolToInt(asset.Fixed_denominations),
						boolToInt(asset.Issued_by_definer_only),
						boolToInt(asset.Cosigned_by_definer),
						boolToInt(asset.Spender_attested),
						objToJsonOrNull(asset.Issue_condition),
						objToJsonOrNull(asset.Transfer_condition),
					})
					if asset.Attestors != nil {
						for j, _ := range asset.Attestors {
							conn.AddQuery("INSERT INTO asset_attestors (unit, message_index, asset, attestor_address) VALUES(?,?,?,?)", DBParamsT{
								objUnit.Unit,
								i,
								objUnit.Unit,
								asset.Attestors[j],
							})
						}
					}
					if asset.Denominations != nil {
						for j, _ := range asset.Denominations {
							conn.AddQuery("INSERT INTO asset_denominations (asset, denomination, count_coins) VALUES(?,?,?)", DBParamsT{
								objUnit.Unit,
								asset.Denominations[j].Denomination,
								asset.Denominations[j].Count_coins,
							})
						}
					}
					//break

				case "asset_attestors":
					asset_attestors := message.Payload
					for j := range asset_attestors.Attestors {
						conn.AddQuery("INSERT INTO asset_attestors (unit, message_index, asset, attestor_address) VALUES(?,?,?,?)", DBParamsT{
							objUnit.Unit,
							i,
							asset_attestors.Asset,
							asset_attestors.Attestors[j],
						})
					}
					//break

				case "data_feed":
					// [tbd] must reparse entire incoming message
					// [tbd] in order to get Payload in data feed format
					data := message.Payload.DataFeed
					for feed_name, value := range data {
						field_name := ""
						switch value.(type) {
						case string:
							field_name = "`value`"
						case int:
							field_name = "int_value"
						default:
							panic("Payload.(data_feed): unexpected value type")
						}
						conn.AddQuery("INSERT INTO data_feeds (unit, message_index, feed_name, "+field_name+") VALUES(?,?,?,?)", DBParamsT{
							objUnit.Unit,
							i,
							feed_name,
							value,
						})
					}
					//break

				case "payment":
					// we'll add inputs/outputs later because we need to read the payer address
					// from src outputs, and it's inconvenient to read it synchronously
					//break

				}
				// switch message.app
			}
			// inline

//			if "spend_proofs" in message {
			if message.Spend_proofs != nil {
//				for j := 0; j < len(message.Spend_proofs); j++ {
//					objSpendProof := message.Spend_proofs[j]
				for j, objSpendProof := range message.Spend_proofs {
					_address := objSpendProof.Address
					if _address.IsNull() { _address = arrAuthorAddresses[0] }
					conn.AddQuery("INSERT INTO spend_proofs (unit, message_index, spend_proof_index, spend_proof, address) VALUES(?,?,?,?,?)", DBParamsT{
						objUnit.Unit,
						i,
						j,
						objSpendProof.Spend_proof,
//						objSpendProof.Address || arrAuthorAddresses[0],
						_address,
					})
				}
			}
		}
	}
	
//	if "earned_headers_commission_recipients" in objUnit {
	if objUnit.Earned_headers_commission_recipients != nil {
//		for i := 0; i < len(objUnit.Earned_headers_commission_recipients); i++ {
//			recipient := objUnit.Earned_headers_commission_recipients[i]
		for _, recipient := range objUnit.Earned_headers_commission_recipients {
			conn.AddQuery("INSERT INTO earned_headers_commission_recipients (unit, address, earned_headers_commission_share) VALUES(?,?,?)", DBParamsT{
				objUnit.Unit,
				recipient.Address,
				recipient.Earned_headers_commission_share,
			})
		}
	}
	
//	my_best_parent_unit := {*init:null*}
	var my_best_parent_unit UnitT
	
//	objNewUnitProps := [*ObjectExpression*]
	objNewUnitProps := XPropsT{
		Unit: objUnit.Unit,
		PropsT: PropsT{
			Level: LevelT_Null,
			Latest_included_mc_index: MCIndexT_Null,
			Main_chain_index: MCIndexT_Null,
			Is_on_main_chain: 0,
			Is_free: 1,
			Is_stable: 0,
			Witnessed_level: LevelT_Null,
		},
		Parent_units: objUnit.Parent_units,
	}
	if bGenesis {
		objNewUnitProps.Level = 0
		objNewUnitProps.Main_chain_index = 0
		objNewUnitProps.Is_on_main_chain = 1
		objNewUnitProps.Is_stable = 1
		objNewUnitProps.Witnessed_level = 0
	}

//	determineInputAddressFromSrcOutput_sync = func (asset AssetT, denomination denominationT, input inputT) AddressT {
	determineInputAddressFromSrcOutput_sync = func (asset AssetT, denomination DenominationT, input InputT) refAddressT {
/**
		rows := /* await * /
		conn.Query_sync("SELECT address, denomination, asset FROM outputs WHERE unit=? AND message_index=? AND output_index=?", DBParamsT{
			input.Unit,
			input.Message_index,
			input.Output_index,
		})
 **/
		rcvr := db.AddressDenominationAssetsReceiver{}
		conn.MustQuery("SELECT address, denomination, asset FROM outputs WHERE unit=? AND message_index=? AND output_index=?", DBParamsT{
			input.Unit,
			input.Message_index,
			input.Output_index,
		}, &rcvr)
		rows := rcvr.Rows
		// << flattened continuation for conn.query:237:3
		if len(rows) > 1 {
			_core.Throw("multiple src outputs found")
		}
		if len(rows) == 0 {
//			if conf.bLight {
			if conf.IsLight {
				// it's normal that a light client doesn't store the previous output
				// :: flattened return for return handleAddress(null);
				return nil
			} else {
				_core.Throw("src output not found")
			}
		}
		row := rows[0]
//		if ! ! asset && ! row.asset || asset == row.asset {
		if ! (asset.IsNull() && row.Asset.IsNull() || asset == row.Asset) {
			_core.Throw("asset doesn't match")
		}
//		if denomination != row.denomination {
		if denomination != row.Denomination {
			_core.Throw("denomination doesn't match")
		}
//		address := row.address
		address := row.Address
//		if arrAuthorAddresses.indexOf(address) == - 1 {
		if arrAuthorAddresses.IndexOf(address) == - 1 {
			_core.Throw("src output address not among authors")
		}
		// :: flattened return for handleAddress(address);
//		return address
		return &address
		// >> flattened continuation for conn.query:237:3
	}
	
//	addInlinePaymentQueries_sync = func ()  {
	addInlinePaymentQueries_sync = func () ErrorT {
		// :: flattened return for cb(async.forEachOfSeries(objUnit.messages, function (message, i) {
		// ** need 0 return(s) instead of 1
		return (func () ErrorT {
		  // :: inlined async.eachOfSeries:263:3
//		  for message, i := range objUnit.Messages {
		  for i, message := range objUnit.Messages {
//		    _err := (func (message messageT, i iT) ErrorT {
		    _err := (func (message MessageT, i int) ErrorT {
		    	if message.Payload_location != "inline" {
		    		// :: flattened return for return cb2();
		    		// ** need 1 return(s) instead of 0
//		    		return 
		    		return nil
		    	}
		    	payload := message.Payload
		    	if message.App != "payment" {
		    		// :: flattened return for return cb2();
		    		// ** need 1 return(s) instead of 0
//		    		return 
		    		return nil
		    	}
		    	
//		    	denomination := payload.Denomination || 1
		    	denomination := payload.Denomination
			if denomination.IsNull() { denomination = 1 }
		    	
		    	(func () ErrorT {
		    	  // :: inlined async.eachOfSeries:274:5
//		    	  for input, j := range payload.Inputs {
		    	  for j, input := range payload.Inputs {
//		    	    _err := (func (input inputT, j jT) ErrorT {
		    	    _err := (func (input InputT, j int) ErrorT {
				//console.Log("j %d input %#v", j, input)

		    	    	var(
//		    	    		determineInputAddress_sync func () AddressT
		    	    		determineInputAddress_sync func () refAddressT
		    	    	)

/**		    	    	
		    	    	type := input.Type || "transfer"
		    	    	src_unit := (type == "transfer" ? input.Unit: nil)
		    	    	src_message_index := (type == "transfer" ? input.Message_index: nil)
		    	    	src_output_index := (type == "transfer" ? input.Output_index: nil)
		    	    	from_main_chain_index := (type == "witnessing" || type == "headers_commission" ? input.From_main_chain_index: nil)
		    	    	to_main_chain_index := (type == "witnessing" || type == "headers_commission" ? input.To_main_chain_index: nil)
 **/
				_type := input.Type
				src_unit := input.Unit
		    	    	src_message_index := input.Message_index
		    	    	src_output_index := input.Output_index
		    	    	from_main_chain_index := input.From_main_chain_index
		    	    	to_main_chain_index := input.To_main_chain_index

				if len(_type) == 0 {
					_type = "transfer"
				}
				if !(_type == "transfer") {
					src_unit = UnitT_Null
			    	    	src_message_index = -1
			    	    	src_output_index = -1
				}
				if !(_type == "witnessing" || _type == "headers_commission") {
			    	    	from_main_chain_index = MCIndexT_Null
			    	    	to_main_chain_index = MCIndexT_Null
				}

//		    	    	determineInputAddress_sync = func () AddressT {
		    	    	determineInputAddress_sync = func () refAddressT {
		    	    		if _type == "headers_commission" || _type == "witnessing" || _type == "issue" {
		    	    			// :: flattened return for return handleAddress(arrAuthorAddresses.length === 1 ? arrAuthorAddresses[0] : input.Address);
//		    	    			if len(arrAuthorAddresses) == 1 { return arrAuthorAddresses[0] }
		    	    			if len(arrAuthorAddresses) == 1 { return &arrAuthorAddresses[0] }
//						return input.Address
						return &input.Address
		    	    		}
		    	    		// hereafter, transfer
		    	    		if len(arrAuthorAddresses) == 1 {
		    	    			// :: flattened return for return handleAddress(arrAuthorAddresses[0]);
//		    	    			return arrAuthorAddresses[0]
		    	    			return &arrAuthorAddresses[0]
		    	    		}
		    	    		// :: flattened return for handleAddress(determineInputAddressFromSrcOutput(payload.asset, denomination, input));
//		    	    		return /* await */
		    	    		return determineInputAddressFromSrcOutput_sync(payload.Asset, denomination, input)
		    	    	}
		    	    	address := /* await */
		    	    	determineInputAddress_sync()
		    	    	// << flattened continuation for determineInputAddress:293:7
		    	    	bDSIs := false
//		    	    	for ds, _ := range objValidationState.arrDoubleSpendInputs {
		    	    	for _, ds := range objValidationState.ArrDoubleSpendInputs {
//		    	    		if ds.message_index == i && ds.input_index == j { bDSIs = true; break }
		    	    		if ds.message_index == MessageIndexT(i) && ds.input_index == InputIndexT(j) { bDSIs = true; break }
		    	    	}
//		    	    	is_unique := 1
		    	    	is_unique := interface{}(1)
		    	    	if bDSIs {
		    	    		is_unique = nil
		    	    	}
		    	    	conn.AddQuery("INSERT INTO inputs \n" +
		    	    		"	(unit, message_index, input_index, type, \n" +
		    	    		"	src_unit, src_message_index, src_output_index, \n" +
		    	    		"	from_main_chain_index, to_main_chain_index, \n" +
		    	    		"	denomination, amount, serial_number, \n" +
		    	    		"	asset, is_unique, address) VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)", DBParamsT{
		    	    		objUnit.Unit,
		    	    		i,
		    	    		j,
		    	    		_type,
//		    	    		src_unit,
		    	    		src_unit.OrNull(),
		    	    		src_message_index,
		    	    		src_output_index,
//		    	    		from_main_chain_index,
		    	    		from_main_chain_index.OrNull(),
//		    	    		to_main_chain_index,
		    	    		to_main_chain_index.OrNull(),
		    	    		denomination,
		    	    		input.Amount,
//		    	    		input.Serial_number,
		    	    		input.Serial_number.OrNull(),
//		    	    		payload.Asset,
		    	    		payload.Asset.OrNull(),
		    	    		is_unique,
		    	    		*address,
		    	    	})
//		    	    	[*SwitchStatement*]
				switch _type {

				case "transfer":
					conn.AddQuery("UPDATE outputs SET is_spent=1 WHERE unit=? AND message_index=? AND output_index=?", DBParamsT{
						src_unit,
						src_message_index,
						src_output_index,
					})
					//break

				case "headers_commission":
					fallthrough
				case "witnessing":
					table := _type + "_outputs"
					conn.AddQuery("UPDATE "+table+" SET is_spent=1 \n"+
						"WHERE main_chain_index>=? AND main_chain_index<=? AND address=?", DBParamsT{
						from_main_chain_index,
						to_main_chain_index,
						*address,
					})
					//break

				}
		    	    	// :: flattened return for cb3();
		    	    	// ** need 1 return(s) instead of 0
//		    	    	return 
		    	    	return nil
		    	    	// >> flattened continuation for determineInputAddress:293:7
		    	    })(input, j)
		    	    if _err != nil { return _err }
		    	  }
		    	  return nil
		    	})()
		    	// << flattened continuation for async.forEachOfSeries:274:5
		    	for j := 0; j < len(payload.Outputs); j++ {
		    		output := payload.Outputs[j]
		    		// we set is_serial=1 for public payments as we check that their inputs are stable and serial before spending, 
		    		// therefore it is impossible to have a nonserial in the middle of the chain (but possible for private payments)
		    		conn.AddQuery("INSERT INTO outputs \n" +
		    			"	(unit, message_index, output_index, address, amount, asset, denomination, is_serial) VALUES(?,?,?,?,?,?,?,1)", DBParamsT{
		    			objUnit.Unit,
		    			i,
		    			j,
		    			output.Address,
//		    			parseInt(output.Amount),
		    			(output.Amount),
//		    			payload.Asset,
		    			payload.Asset.OrNull(),
		    			denomination,
		    		})
		    	}
		    	// :: flattened return for cb2();
		    	// ** need 1 return(s) instead of 0
//		    	return 
		    	return nil
		    	// >> flattened continuation for async.forEachOfSeries:274:5
		    })(message, i)
		    if _err != nil { return _err }
		  }
		  return nil
		})()
	}
	
	updateBestParent_sync = func () ErrorT {
/**
		rows := /* await * /
		conn.Query_sync("SELECT unit \n" +
 **/
		rcvr := db.UnitsReceiver{}
		queryParams := DBParamsT{}
		pusSql := queryParams.AddUnits(objUnit.Parent_units)
		queryParams = append(queryParams,
			objUnit.Witness_list_unit,
			objUnit.Unit,
			objUnit.Witness_list_unit,
			constants.COUNT_WITNESSES - constants.MAX_WITNESS_LIST_MUTATIONS)
		conn.MustQuery("SELECT unit \n" +
			"	FROM units AS parent_units \n" +
			"	WHERE unit IN(" + pusSql + ") \n" +
			"		AND (witness_list_unit=? OR ( \n" +
			"			SELECT COUNT(*) \n" +
			"			FROM unit_witnesses \n" +
			"			JOIN unit_witnesses AS parent_witnesses USING(address) \n" +
			"			WHERE parent_witnesses.unit IN(parent_units.unit, parent_units.witness_list_unit) \n" +
			"				AND unit_witnesses.unit IN(?, ?) \n" +
			"		)>=?) \n" +
			"	ORDER BY witnessed_level DESC, \n" +
			"		level-witnessed_level ASC, \n" +
			"		unit ASC \n" +
			"	LIMIT 1", queryParams, &rcvr)
		rows := rcvr.Rows
		// << flattened continuation for conn.query:346:3
		if len(rows) != 1 {
			_core.Throw("zero or more than one best parent unit?")
		}
		my_best_parent_unit = rows[0].Unit
		if my_best_parent_unit != objValidationState.Best_parent_unit {
//			throwError("different best parents, validation: " + objValidationState.Best_parent_unit + ", writer: " + my_best_parent_unit)
			throwError(fmt.Sprintf("different best parents, validation: %s, writer: %s", objValidationState.Best_parent_unit, my_best_parent_unit))
		}
/**
		/* await * /
		conn.Query_sync("UPDATE units SET best_parent_unit=? WHERE unit=?", DBParamsT{
			my_best_parent_unit,
			objUnit.Unit,
		})
 **/
		conn.MustExec("UPDATE units SET best_parent_unit=? WHERE unit=?", DBParamsT{
			my_best_parent_unit,
			objUnit.Unit,
		})
		// << flattened continuation for conn.query:369:5
		// :: flattened return for cb();
		// ** need 1 return(s) instead of 0
//		return 
		return nil
		// >> flattened continuation for conn.query:369:5
		// >> flattened continuation for conn.query:346:3
	}
	
	determineMaxLevel_sync = func () LevelT {
//		max_level := 0
		max_level := LevelT(0)
		(func () ErrorT {
		  // :: inlined async.each:376:3 !! [tbd] finish this
//		  for parent_unit := range objUnit.parent_units {
		  for _, parent_unit := range objUnit.Parent_units {
		    _err := (func (parent_unit UnitT) ErrorT {
		    	props := /* await */
//		    	storage.readStaticUnitProps_sync(conn, parent_unit)
		    	storage.ReadStaticUnitProps_sync(conn, parent_unit)
		    	// << flattened continuation for storage.readStaticUnitProps:379:5
//		    	if props.level > max_level {
//		    		max_level = props.level
		    	if props.Level > max_level {
		    		max_level = props.Level
		    	}
		    	// :: flattened return for cb();
		    	// ** need 1 return(s) instead of 0
//		    	return 
		    	return nil
		    	// >> flattened continuation for storage.readStaticUnitProps:379:5
		    })(parent_unit)
		    if _err != nil { return _err }
		  }
		  return nil
		})()
		// << flattened continuation for async.each:376:3
		// :: flattened return for handleMaxLevel(max_level);
		// ** need 0 return(s) instead of 1
		return max_level
		// >> flattened continuation for async.each:376:3
	}
	
	updateLevel_sync = func () ErrorT {
/**
		rows := /* await * /
		conn.Query_sync("SELECT MAX(level) AS max_level FROM units WHERE unit IN(?)", DBParamsT{
			objUnit.Parent_units,
		})
 **/
		rcvr := db.MaxLevelsReceiver{}
		queryParams := DBParamsT{}
		pusSql := queryParams.AddUnits(objUnit.Parent_units)
		conn.MustQuery("SELECT MAX(level) AS max_level FROM units WHERE unit IN(" + pusSql + ")", queryParams, &rcvr)
		rows := rcvr.Rows
		// << flattened continuation for conn.query:392:3
		if len(rows) != 1 {
			_core.Throw("not a single max level?")
		}
		max_level := /* await */
		determineMaxLevel_sync()
		// << flattened continuation for determineMaxLevel:395:4
//		if max_level != rows[0].max_level {
		if max_level != rows[0].Max_level {
//			throwError("different max level, sql: " + rows[0].max_level + ", props: " + max_level)
			throwError(fmt.Sprintf("different max level, sql: %d, props: %d", rows[0].Max_level, max_level))
		}
//		objNewUnitProps.level = max_level + 1
		objNewUnitProps.Level = LevelT(max_level + 1)
/**
		/* await * /
		conn.Query_sync("UPDATE units SET level=? WHERE unit=?", DBParamsT{
			rows[0].Max_level + 1,
			objUnit.Unit,
		})
 **/
		conn.MustExec("UPDATE units SET level=? WHERE unit=?", DBParamsT{
			rows[0].Max_level + 1,
			objUnit.Unit,
		})
		// << flattened continuation for conn.query:399:5
		// :: flattened return for cb();
		// ** need 1 return(s) instead of 0
//		return 
		return nil
		// >> flattened continuation for conn.query:399:5
		// >> flattened continuation for determineMaxLevel:395:4
		// >> flattened continuation for conn.query:392:3
	}
	
	
	updateWitnessedLevel_sync = func () ErrorT {
//		if objUnit.Witnesses {
		if objUnit.Witnesses != nil {
//		if len(objUnit.Witnesses) != 0 {
			// :: flattened return for cb(updateWitnessedLevelByWitnesslist(objUnit.Witnesses));
//			return /* await */
//			updateWitnessedLevelByWitnesslist_sync(objUnit.Witnesses)
			return updateWitnessedLevelByWitnesslist_sync(objUnit.Witnesses)
		} else {
			arrWitnesses := /* await */
//			storage.readWitnessList_sync(conn, objUnit.Witness_list_unit)
			storage.ReadWitnessList_sync(conn, objUnit.Witness_list_unit, false)
			// << flattened continuation for storage.readWitnessList:411:4
			// :: flattened return for cb(updateWitnessedLevelByWitnesslist(arrWitnesses));
//			return /* await */
//			updateWitnessedLevelByWitnesslist_sync(arrWitnesses)
			return updateWitnessedLevelByWitnesslist_sync(arrWitnesses)
			// >> flattened continuation for storage.readWitnessList:411:4
		}
	}
	
	// The level at which we collect at least 7 distinct witnesses while walking up the main chain from our unit.
	// The unit itself is not counted even if it is authored by a witness
	updateWitnessedLevelByWitnesslist_sync = func (arrWitnesses AddressesT) ErrorT {
		var(
//			setWitnessedLevel func (witnessed_level witnessed_levelT) 
			setWitnessedLevel func (witnessed_level LevelT) ErrorT
//			addWitnessesAndGoUp func (start_unit UnitT) (int, UnitT)
			addWitnessesAndGoUp func (start_unit UnitT) ErrorT
		)
		
		arrCollectedWitnesses := AddressesT{}
		
//		setWitnessedLevel = func (witnessed_level witnessed_levelT)  {
		setWitnessedLevel = func (witnessed_level LevelT) ErrorT {
			profiler.Start()
			if witnessed_level != objValidationState.Witnessed_level {
//				throwError("different witnessed levels, validation: " + objValidationState.witnessed_level + ", writer: " + witnessed_level)
				throwError(fmt.Sprintf("different witnessed levels, validation: %d, writer: %d", objValidationState.Witnessed_level, witnessed_level))
			}
//			objNewUnitProps.witnessed_level = witnessed_level
			objNewUnitProps.Witnessed_level = witnessed_level
/**
			/* await * /
			conn.Query_sync("UPDATE units SET witnessed_level=? WHERE unit=?", DBParamsT{
				witnessed_level,
				objUnit.Unit,
			})
 **/
			conn.MustExec("UPDATE units SET witnessed_level=? WHERE unit=?", DBParamsT{
				witnessed_level,
				objUnit.Unit,
			})
			// << flattened continuation for conn.query:426:4
			profiler.Stop("write-wl-update")
			// :: flattened return for cb();
			// ** need 1 return(s) instead of 0
//			return 
			return nil
			// >> flattened continuation for conn.query:426:4
		}
		
//		addWitnessesAndGoUp = func (start_unit UnitT) (int, UnitT) {
		addWitnessesAndGoUp = func (start_unit UnitT) ErrorT {
			profiler.Start()
			props := /* await */
			storage.ReadStaticUnitProps_sync(conn, start_unit)
			// << flattened continuation for storage.readStaticUnitProps:434:4
			profiler.Stop("write-wl-select-bp")
//			best_parent_unit := props.best_parent_unit
			best_parent_unit := props.Best_parent_unit
//			level := props.level
			level := props.Level
//			if level == nil {
			if level.IsNull() {
//			if level == -1 {
				_core.Throw("null level in updateWitnessedLevel")
			}
			if level == 0 {
				// genesis
//				setWitnessedLevel(0)
//				return
				return setWitnessedLevel(0)
			}
			profiler.Start()
			arrAuthors := /* await */
			storage.ReadUnitAuthors_sync(conn, start_unit)
			// << flattened continuation for storage.readUnitAuthors:443:5
			profiler.Stop("write-wl-select-authors")
			profiler.Start()
//			for i := 0; i < len(arrAuthors); i++ {
//				address := arrAuthors[i]
			for _, address := range(arrAuthors) {
//				if arrWitnesses.indexOf(address) != - 1 && arrCollectedWitnesses.indexOf(address) == - 1 {
				if arrWitnesses.IndexOf(address) != - 1 && arrCollectedWitnesses.IndexOf(address) == - 1 {
					arrCollectedWitnesses = append(arrCollectedWitnesses, address)
				}
			}
			profiler.Stop("write-wl-search")
			if len(arrCollectedWitnesses) >= constants.MAJORITY_OF_WITNESSES {
//				setWitnessedLevel(level)
//				return
				return setWitnessedLevel(level)
			}
//			addWitnessesAndGoUp(best_parent_unit)
			return addWitnessesAndGoUp(best_parent_unit)
			// >> flattened continuation for storage.readUnitAuthors:443:5
			// >> flattened continuation for storage.readStaticUnitProps:434:4
		}
		
		profiler.Stop("write-update")
//		addWitnessesAndGoUp(my_best_parent_unit)
		return addWitnessesAndGoUp(my_best_parent_unit)
	}
	
	// [tbd] move variable declaration up before function declarations
//	objNewUnitProps := [*ObjectExpression*]

	unlock := /* await */
//	mutex.lock_sync({*ArrayExpression*})
	mutex.Lock_sync([]string{"write"})
	// << flattened continuation for mutex.lock:477:2
//	console.Log("got lock to write " + objUnit.unit)
	console.Log("got lock to write %s", objUnit.Unit)
//	storage.assocUnstableUnits[objUnit.unit] = objNewUnitProps
	storage.AssocUnstableUnits[objUnit.Unit] = &objNewUnitProps
	/* await */
	addInlinePaymentQueries_sync()
	// << flattened continuation for addInlinePaymentQueries:480:3
/**
	(func () ErrorT {
	  // :: inlined async.series:481:4
//	  for _f := range arrQueries {
	  for _, _f := range arrQueries {
	    if _err := _f() ; _err != nil { return _err }
	  }
	  return nil
	})()
 **/
	conn.BeginTransaction()
	conn.ExecuteAddedQuerys()
	// << flattened continuation for async.series:481:4
	profiler.Stop("write-raw")
	profiler.Start()
	arrOps := AsyncFunctorsT{}
//	if objUnit.parent_units {
	if objUnit.Parent_units != nil {
//	if 0 < len(objUnit.Parent_units) {
//		if ! conf.bLight {
		if ! conf.IsLight {
//			arrOps = append(arrOps, updateBestParent)
			arrOps = append(arrOps, updateBestParent_sync)
//			arrOps = append(arrOps, updateLevel)
			arrOps = append(arrOps, updateLevel_sync)
//			arrOps = append(arrOps, updateWitnessedLevel)
			arrOps = append(arrOps, updateWitnessedLevel_sync)
			arrOps = append(arrOps, func () ErrorT {
//				console.Log("updating MC after adding " + objUnit.unit)
				console.Log("updating MC after adding %s", objUnit.Unit)
				/* await */
//				main_chain.updateMainChain_sync(conn, nil, objUnit.unit)
				main_chain.UpdateMainChain_sync(conn, UnitT_Null, objUnit.Unit)
				// << flattened continuation for main_chain.updateMainChain:492:8
				// :: flattened return for cb();
				// ** need 1 return(s) instead of 0
//				return 
				return nil
				// >> flattened continuation for main_chain.updateMainChain:492:8
			})
		}
//		if preCommitCallback {
		if preCommitCallback_sync != nil {
			arrOps = append(arrOps, func () ErrorT {
				console.Log("executing pre-commit callback")
				/* await */
				preCommitCallback_sync(conn)
				// << flattened continuation for preCommitCallback:498:8
				// :: flattened return for cb();
				// ** need 1 return(s) instead of 0
//				return 
				return nil
				// >> flattened continuation for preCommitCallback:498:8
			})
		}
	}
	err := (func () ErrorT {
	  // :: inlined async.series:501:5
//	  for _f := range arrOps {
	  for _, _f := range arrOps {
	    if _err := _f() ; _err != nil { return _err }
	  }
	  return nil
	})()
	// << flattened continuation for async.series:501:5

	// [!!!] force rollback
//!!	if err == nil { err = errors.New("forced ROLLBACK") }

	profiler.Start()
	/* await */
//	conn.query_sync((err ? "ROLLBACK": "COMMIT"))
	if err == nil {
//		conn.Query_sync("COMMIT")
//--		conn.MustExec("COMMIT", nil)
		conn.Commit()
	} else {
//		conn.Query_sync("ROLLBACK")
//--		conn.MustExec("ROLLBACK", nil)
		conn.Rollback()
	}
	// << flattened continuation for conn.query:503:6
//	conn.release()
	conn.Release()
//	console.Log((err ? err + ", therefore rolled back unit ": "committed unit ") + objUnit.unit)
	if err == nil {
//		console.Log("committed unit " + objUnit.unit)
		console.Log("committed unit %s", objUnit.Unit)
	} else {
//		console.Log(err + ", therefore rolled back unit " + objUnit.unit)
		console.Log("%s, therefore rolled back unit %s", err, objUnit.Unit)
	}
	profiler.Stop("write-commit")
	profiler.Increment()
//	if err {
	if err != nil {
		/* await */
		storage.ResetUnstableUnits_sync()
		// << flattened continuation for storage.resetUnstableUnits:509:8
		unlock()
		// >> flattened continuation for storage.resetUnstableUnits:509:8
	} else {
		unlock()
	}
//	if ! err {
	if err == nil {
		eventBus.Emit(fmt.Sprintf("saved_unit-%s", objUnit.Unit), objJoint)
	}
//	if onDone {
	if true {
		// :: flattened return for onDone(err);
		return err
	}
	count_writes++
	if conf.Storage == "sqlite" {
		updateSqliteStats()
	}
	// >> flattened continuation for conn.query:503:6
	// >> flattened continuation for async.series:501:5
	// >> flattened continuation for async.series:481:4
	// >> flattened continuation for addInlinePaymentQueries:480:3
	// >> flattened continuation for mutex.lock:477:2
	// >> flattened continuation for db.takeConnectionFromPool:24:1
	return nil
}

func updateSqliteStats()  {
}


const _readCountOfAnalyzedUnits_sync = `
func readCountOfAnalyzedUnits_sync()  {
	if count_units_in_prev_analyze {
		// :: flattened return for return handleCount(count_units_in_prev_analyze);
		// ** need 0 return(s) instead of 1
		return count_units_in_prev_analyze
	}
	rows := /* await */
	db.Query_sync("SELECT * FROM sqlite_master WHERE type='table' AND name='sqlite_stat1'")
	// << flattened continuation for db.query:531:1
	if len(rows) == 0 {
		// :: flattened return for return handleCount(0);
		// ** need 0 return(s) instead of 1
		return 0
	}
//	rows := /* await */
	rows = /* await */
	db.Query_sync("SELECT stat FROM sqlite_stat1 WHERE tbl='units' AND idx='sqlite_autoindex_units_1'")
	// << flattened continuation for db.query:534:2
	if len(rows) != 1 {
		console.Log("no stat for sqlite_autoindex_units_1")
		// :: flattened return for return handleCount(0);
		// ** need 0 return(s) instead of 1
		return 0
	}
	// :: flattened return for handleCount(parseInt(rows[0].stat.split(' ')[0]));
	// ** need 0 return(s) instead of 1
	return parseInt(rows[0].stat.split(" ")[0])
	// >> flattened continuation for db.query:534:2
	// >> flattened continuation for db.query:531:1
}

start_time := 0
prev_time := 0

// update stats for query planner
func updateSqliteStats()  {
	if count_writes == 1 {
		start_time = Date.now()
		prev_time = Date.now()
	}
	if count_writes % 100 != 0 {
		return 
	}
	if count_writes % 1000 == 0 {
		total_time := Date.now() - start_time / 1000
		recent_time := Date.now() - prev_time / 1000
		recent_tps := 1000 / recent_time
		avg_tps := count_writes / total_time
		prev_time = Date.now()
	}
	rows := /* await */
	db.query_sync("SELECT MAX(rowid) AS count_units FROM units")
	// << flattened continuation for db.query:562:1
	count_units := rows[0].count_units
	if count_units > 500000 {
		// the db is too big
		return 
	}
	count_analyzed_units := /* await * /
	readCountOfAnalyzedUnits_sync()
	// << flattened continuation for readCountOfAnalyzedUnits:566:2
	console.Log("count analyzed units: %d", count_analyzed_units)
	if count_units < 2 * count_analyzed_units {
		return 
	}
	count_units_in_prev_analyze = count_units
	console.Log("will update sqlite stats")
	/* await * /
	db.query_sync("ANALYZE")
	// << flattened continuation for db.query:572:3
	/* await * /
	db.query_sync("ANALYZE sqlite_master")
	// << flattened continuation for db.query:573:4
	console.Log("sqlite stats updated")
	// >> flattened continuation for db.query:573:4
	// >> flattened continuation for db.query:572:3
	// >> flattened continuation for readCountOfAnalyzedUnits:566:2
	// >> flattened continuation for db.query:562:1
}
`

func throwError(msg string) {
//	if typeof window == "undefined" {
	if true {
		_core.Throw(msg)
	} else {
//		eventBus.emit("nonfatal_error", msg, [*NewExpression*])
		eventBus.Emit("nonfatal_error", msg, nil)
	}
}

//exports.saveJoint = saveJoint


// converted golang end

